{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_lstm_generate_text",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adi0229/ML-DL/blob/master/keras_lstm_generate_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEwX9bC0p8Yn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "15a53069-cdcd-4df6-a4eb-5861791f5779"
      },
      "source": [
        "# language model by keras\n",
        "\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaOa5kLO6doM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b90264eb-80b9-42ea-d9f1-de501fdd3101"
      },
      "source": [
        "keras.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpSNmUNTrV-5",
        "colab_type": "code",
        "outputId": "595f4f8e-dca1-4068-8bfd-4f4abf09bab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3it09twrqvi",
        "colab_type": "code",
        "outputId": "778508af-7231-4190-edbb-3103e207f683",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import os\n",
        "\n",
        "os.getcwd()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72Jcp1E5sG7z",
        "colab_type": "text"
      },
      "source": [
        "更换语料库美剧《老友记》剧本"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRx9BTQhqBOq",
        "colab_type": "code",
        "outputId": "5f77ffda-e4d2-4049-9a2e-8d3e6b229922",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "text = open('/content/gdrive/My Drive/friends.txt').read().lower()\n",
        "print(\"Coprus length:\", len(text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coprus length: 4870645\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAim22Gr63nW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7024966d-1bee-44a8-80c6-a4fdee371171"
      },
      "source": [
        "type(text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIMLyhCj7Bas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = text[:300000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKzz9gPTryw9",
        "colab_type": "code",
        "outputId": "2c65bf00-9769-4856-8367-f7e05eb119e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "maxlen = 60\n",
        "step = 3\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0,len(text)- maxlen, step):\n",
        "  sentences.append(text[i:i+ maxlen])\n",
        "  next_chars.append(text[i+ maxlen])\n",
        "\n",
        "print('Number of sequences:',len(sentences))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 99980\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BW8p0ngws5IG",
        "colab_type": "code",
        "outputId": "a37d7432-90b4-483d-c7b7-8e7cf6eac7d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "sentences[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['101 the one where monica gets a new roommate (the pilot-the ',\n",
              " ' the one where monica gets a new roommate (the pilot-the unc',\n",
              " 'e one where monica gets a new roommate (the pilot-the uncut ',\n",
              " 'ne where monica gets a new roommate (the pilot-the uncut ver',\n",
              " 'where monica gets a new roommate (the pilot-the uncut versio',\n",
              " 're monica gets a new roommate (the pilot-the uncut version)\\n',\n",
              " 'monica gets a new roommate (the pilot-the uncut version)\\n[sc',\n",
              " 'ica gets a new roommate (the pilot-the uncut version)\\n[scene',\n",
              " ' gets a new roommate (the pilot-the uncut version)\\n[scene: c',\n",
              " 'ts a new roommate (the pilot-the uncut version)\\n[scene: cent']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwEKgjO8tAIq",
        "colab_type": "code",
        "outputId": "7acc3e9d-35b7-421c-9fe4-6bcf15ab015a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "next_chars[:10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['u', 'u', 'v', 's', 'n', '[', 'e', ':', 'e', 'r']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1zPuh7EtFVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYvPUL2ntiGq",
        "colab_type": "code",
        "outputId": "ff224acf-380f-4d1e-c1aa-dcb32b009aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "chars[:10] # unique chars in the corpus"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n', ' ', '!', '\"', '$', '&', \"'\", '(', ')', ',']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlWk327htkBT",
        "colab_type": "code",
        "outputId": "aa93b59a-3baa-4ead-d92f-5453170b4b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Unique characters:\", len(chars))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique characters: 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WG5IyAB0t1ht",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_indices = dict((char, chars.index(char)) for char in chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIb9TJHiuSi0",
        "colab_type": "code",
        "outputId": "e8d577cd-8709-4768-8e01-968c3fba35db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "char_indices"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '$': 4,\n",
              " '&': 5,\n",
              " \"'\": 6,\n",
              " '(': 7,\n",
              " ')': 8,\n",
              " ',': 9,\n",
              " '-': 10,\n",
              " '.': 11,\n",
              " '/': 12,\n",
              " '0': 13,\n",
              " '1': 14,\n",
              " '2': 15,\n",
              " '3': 16,\n",
              " '4': 17,\n",
              " '5': 18,\n",
              " '6': 19,\n",
              " '7': 20,\n",
              " '8': 21,\n",
              " '9': 22,\n",
              " ':': 23,\n",
              " ';': 24,\n",
              " '?': 25,\n",
              " '[': 26,\n",
              " ']': 27,\n",
              " 'a': 28,\n",
              " 'b': 29,\n",
              " 'c': 30,\n",
              " 'd': 31,\n",
              " 'e': 32,\n",
              " 'f': 33,\n",
              " 'g': 34,\n",
              " 'h': 35,\n",
              " 'i': 36,\n",
              " 'j': 37,\n",
              " 'k': 38,\n",
              " 'l': 39,\n",
              " 'm': 40,\n",
              " 'n': 41,\n",
              " 'o': 42,\n",
              " 'p': 43,\n",
              " 'q': 44,\n",
              " 'r': 45,\n",
              " 's': 46,\n",
              " 't': 47,\n",
              " 'u': 48,\n",
              " 'v': 49,\n",
              " 'w': 50,\n",
              " 'x': 51,\n",
              " 'y': 52,\n",
              " 'z': 53,\n",
              " '}': 54,\n",
              " '—': 55,\n",
              " '’': 56}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPAsh6-buWRY",
        "colab_type": "code",
        "outputId": "ee6988d1-6445-414e-aa0e-5c644761fb2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print('Vectorizion...')\n",
        "\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizion...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD4AzMRj0jnM",
        "colab_type": "text"
      },
      "source": [
        "生成文本 \"温度（temperatures）\" 实验……"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjvYO-k11rG3",
        "colab_type": "text"
      },
      "source": [
        "## 构建长短时记忆（LSTM）神经网络"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-e8vXVy1yMP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "a567b175-369d-4044-e773-806012c55817"
      },
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0625 09:37:51.169493 140571770709888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0625 09:37:51.193822 140571770709888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0625 09:37:51.197820 140571770709888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcFcCzyv2M8E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "6e999d25-bb6e-4dd7-ceec-b6b4d56e3447"
      },
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(lr=0.01),loss='categorical_crossentropy')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0625 09:37:51.487029 140571770709888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0625 09:37:51.496897 140571770709888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrwzQvKv2zhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZszRZXVO3jA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d845d876-55c9-4e53-fafd-32a6b744ac0f"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "\n",
        "for epoch in range(1, 20):\n",
        "    print('epoch', epoch)\n",
        "    # Fit the model for 1 epoch on the available training data\n",
        "    model.fit(x, y,\n",
        "              batch_size=128,\n",
        "              epochs=1)\n",
        "\n",
        "    # Select a text seed at random\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated_text = text[start_index: start_index + maxlen]\n",
        "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "    for temperature in [0.1, 0.5, 0.8,]:\n",
        "        print('------ temperature:', temperature)\n",
        "        sys.stdout.write(generated_text)\n",
        "\n",
        "        # We generate 100 characters\n",
        "        for i in range(100):\n",
        "            sampled = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(generated_text):\n",
        "                sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(sampled, verbose=0)[0]\n",
        "            next_index = sample(preds, temperature)\n",
        "            next_char = chars[next_index]\n",
        "\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0625 09:37:51.658761 140571770709888 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0625 09:37:52.468288 140571770709888 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 2.0813\n",
            "--- Generating with seed: \"ever.\n",
            "all: oh, come on! come on!\n",
            "monica: no. not after what \"\n",
            "------ temperature: 0.1\n",
            "ever.\n",
            "all: oh, come on! come on!\n",
            "monica: no. not after what i wants the can the can the can the cand the can the cane the can the can the can she cane the can t\n",
            "------ temperature: 0.5\n",
            " the can the cane the can the can the can she cane the can the ceaing i see oking get are happen she caille the paren the to leas it said on the could the thing\n",
            "------ temperature: 0.8\n",
            " caille the paren the to leas it said on the could the thinge bo pits ick there fino me, nok, some cand wint?\n",
            "monica: monica-eblige tome this men to wealing the\n",
            "epoch 2\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 1.6337\n",
            "--- Generating with seed: \"aves her head? she said that if you want to break the bad bo\"\n",
            "------ temperature: 0.1\n",
            "aves her head? she said that if you want to break the bad book and rachel and ross a sayer and ross a say to a pacent.\n",
            "ross: (something a says to the for a saye\n",
            "------ temperature: 0.5\n",
            "a say to a pacent.\n",
            "ross: (something a says to the for a sayes seeting to moneca to lacked coffromend moneca offerosed suppers about to rosselling a bag of to so\n",
            "------ temperature: 0.8\n",
            " moneca offerosed suppers about to rosselling a bag of to soverter grand.\n",
            "ross: weah, look, and will, aroy, mat sometyent.\n",
            "monica: for a hacked to comer she win\n",
            "epoch 3\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.5049\n",
            "--- Generating with seed: \"t) well.... it is amouz-ing...\n",
            "(phone rings. monica answers \"\n",
            "------ temperature: 0.1\n",
            "t) well.... it is amouz-ing...\n",
            "(phone rings. monica answers it doing to be it on the start.)\n",
            "monica: what it was this this this this this this with this this th\n",
            "------ temperature: 0.5\n",
            " what it was this this this this this this with this this that chandler on her the woopule but and you do this on the to start.  she was chandler and we doing t\n",
            "------ temperature: 0.8\n",
            "ou do this on the to start.  she was chandler and we doing thist chees mad tomend yoursth-flow. whow it have with you fight it. chandler out his her. here, i di\n",
            "epoch 4\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 1.4283\n",
            "--- Generating with seed: \" you called me. i had the most supremely awful day.\n",
            "chandler\"\n",
            "------ temperature: 0.1\n",
            " you called me. i had the most supremely awful day.\n",
            "chandler: oh my god, and the here we does it was the should the book on the should the should her her on the\n",
            "------ temperature: 0.5\n",
            " the should the book on the should the should her her on the to her) oh monica and rachel's seees her) the putter in the to the say, i don't think it's here wha\n",
            "------ temperature: 0.8\n",
            "r) the putter in the to the say, i don't think it's here what have it thing, but over here me where fork and he is a mrome her dearstay that a stop of the slast\n",
            "epoch 5\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.3741\n",
            "--- Generating with seed: \"s) in the cab, on the way over, steve blazed up a doobie.\n",
            "ra\"\n",
            "------ temperature: 0.1\n",
            "s) in the cab, on the way over, steve blazed up a doobie.\n",
            "rachel: oh, i don't know, i don't know, i don't know, i don't know, i don't know, i don't know, i don'\n",
            "------ temperature: 0.5\n",
            "don't know, i don't know, i don't know, i don't know, i don't know, you guys! (to there to him) (she hair) i'm looking to do this it. (to chandler to like think\n",
            "------ temperature: 0.8\n",
            " hair) i'm looking to do this it. (to chandler to like think you is a tothing. you have toight theme because old' lat it the wanna from this. you're gorny, so i\n",
            "epoch 6\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 133s 1ms/step - loss: 1.3385\n",
            "--- Generating with seed: \"ah.\n",
            "chandler: oh, it's nothing, it's a little thing... i hat\"\n",
            "------ temperature: 0.1\n",
            "ah.\n",
            "chandler: oh, it's nothing, it's a little thing... i hate the this with the lead to the to the to the to the to the to the to the to the to the to the to co\n",
            "------ temperature: 0.5\n",
            "o the to the to the to the to the to the to the to the to come of menting they were to ross.\n",
            "chandler: i don't know what the wearen there?\n",
            "chandler: (pars reall\n",
            "------ temperature: 0.8\n",
            "r: i don't know what the wearen there?\n",
            "chandler: (pars really has had bece.)\n",
            "chandler: pheebs, i'm do the lasand her see and not the beee. (she wanns and phoebe\n",
            "epoch 7\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.3070\n",
            "--- Generating with seed: \"ve to show her your peepee.\n",
            "chandler: y'know, i don't see th\"\n",
            "------ temperature: 0.1\n",
            "ve to show her your peepee.\n",
            "chandler: y'know, i don't see the were the were the bally the bally the back of the to the back of the should to the to the stupid t\n",
            "------ temperature: 0.5\n",
            "back of the to the back of the should to the to the stupid to me with ross is there to her are to her and joey and rachel's chandler and rachel are the see the \n",
            "------ temperature: 0.8\n",
            "r and joey and rachel's chandler and rachel are the see the to something of the sand yorked at he cheep and the window with dees) (monica catt)\n",
            "phoebe: (teslees\n",
            "epoch 8\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.2818\n",
            "--- Generating with seed: \"chandler how to swing the pen around his head.]\n",
            "jill: chandl\"\n",
            "------ temperature: 0.1\n",
            "chandler how to swing the pen around his head.]\n",
            "jill: chandler, what about you gonna see the only grat on the parents as ross and rachel and rachel and rachel a\n",
            "------ temperature: 0.5\n",
            "at on the parents as ross and rachel and rachel and rachel and chandler and rachel is going toougers and play and guys modne wish who and where was the moment w\n",
            "------ temperature: 0.8\n",
            " and play and guys modne wish who and where was the moment weares i can sounds on of back.\n",
            "joey: why whiter cham! i know here what around sofactly.\n",
            "rachel: phoe\n",
            "epoch 9\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 130s 1ms/step - loss: 1.2601\n",
            "--- Generating with seed: \" it's working. does this hurt? (presses down elsewhere)\n",
            "stev\"\n",
            "------ temperature: 0.1\n",
            " it's working. does this hurt? (presses down elsewhere)\n",
            "steve: (on the cates) what want the baby and the were the cat.\n",
            "chandler: oh, i want to be not starts to \n",
            "------ temperature: 0.5\n",
            " the were the cat.\n",
            "chandler: oh, i want to be not starts to be in the dited to get the pan. of her head)\n",
            "mrs. geller: yes, it's fine. you could be naked to doin\n",
            "------ temperature: 0.8\n",
            "ead)\n",
            "mrs. geller: yes, it's fine. you could be naked to doings, uh, sandybure that the for the pate of excresly really should be a smother. right other ron. bac\n",
            "epoch 10\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.2429\n",
            "--- Generating with seed: \"buffay, everybody. woo!\n",
            "(enter chandler)\n",
            "chandler: what was \"\n",
            "------ temperature: 0.1\n",
            "buffay, everybody. woo!\n",
            "(enter chandler)\n",
            "chandler: what was the were the woowed me. what all right. you can see what i don't know what you wanna be a starts it.\n",
            "------ temperature: 0.5\n",
            "you can see what i don't know what you wanna be a starts it. (goess such back of the phone, good-loow, the would gare starts work at the really down?\n",
            "joey: (pla\n",
            "------ temperature: 0.8\n",
            "w, the would gare starts work at the really down?\n",
            "joey: (plans run poarrionosting a monica fing play) so ope and suddrr's my one. that was caroluce good!\n",
            "joey: \n",
            "epoch 11\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.2265\n",
            "--- Generating with seed: \"t is the unusual activity. look, they just wanna see if you'\"\n",
            "------ temperature: 0.1\n",
            "t is the unusual activity. look, they just wanna see if you're gonna be see the rest of the call of the call and they she gonna start the cates and they she gon\n",
            "------ temperature: 0.5\n",
            "the call and they she gonna start the cates and they she gonna sing to the table wall go candl back and his mally does i want to stearing with my moment, i wann\n",
            "------ temperature: 0.8\n",
            "and his mally does i want to stearing with my moment, i wanna beard to play.\n",
            "chandler: (hands the pit in an an in my ting... in the table over and carol to the \n",
            "epoch 12\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.2141\n",
            "--- Generating with seed: \"hel: oh ross, you're so great!\n",
            "ross: ohhhh (hugs her and sig\"\n",
            "------ temperature: 0.1\n",
            "hel: oh ross, you're so great!\n",
            "ross: ohhhh (hugs her and siging to the to the to the to the to the to the to the to the to the to the to the to the to the to th\n",
            "------ temperature: 0.5\n",
            "o the to the to the to the to the to the to the to the to the now hand something me and rachel guys are there.]\n",
            "monica: (stips on the termore i"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "s like her. (hand\n",
            "------ temperature: 0.8\n",
            "are there.]\n",
            "monica: (stips on the termore is like her. (hands the others) what about you as and me, and he is?\n",
            "phoebe: you're that every!\n",
            "(kirsy back to the too\n",
            "epoch 13\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 1.1984\n",
            "--- Generating with seed: \"his pop-tart?\n",
            "ross: hey, i might!\n",
            "phoebe: sorry. ..y'know, t\"\n",
            "------ temperature: 0.1\n",
            "his pop-tart?\n",
            "ross: hey, i might!\n",
            "phoebe: sorry. ..y'know, the line and susan any mone with the restaurant, you know what i mean, i mean, i don't know what i do\n",
            "------ temperature: 0.5\n",
            "aurant, you know what i mean, i mean, i don't know what i don't know it. the has the too. i think i mean, and i was the bading to say here?\n",
            "ross: no. so you got\n",
            "------ temperature: 0.8\n",
            "mean, and i was the bading to say here?\n",
            "ross: no. so you got to hold it.\n",
            "ross: how's i could be me.\n",
            "joey: you know. (she doesn't look on his let your guy her re\n",
            "epoch 14\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.1892\n",
            "--- Generating with seed: \"on.\n",
            "monica: ross, that’s my jacket.\n",
            "ross: i know.\n",
            "(rachel gr\"\n",
            "------ temperature: 0.1\n",
            "on.\n",
            "monica: ross, that’s my jacket.\n",
            "ross: i know.\n",
            "(rachel grough on the table watching on the table with an a spotcher and then the to the too the too starts to\n",
            "------ temperature: 0.5\n",
            "with an a spotcher and then the to the too the too starts to the pancle hand on her) monica. (she going her pronofried her head) i'm not the time on a for heard\n",
            "------ temperature: 0.8\n",
            "ing her pronofried her head) i'm not the time on a for heard my gangle!\n",
            "joey: (trying to the couch offrolocen from the stoupx to it.)\n",
            "ross: you mean, and the th\n",
            "epoch 15\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 130s 1ms/step - loss: 1.1778\n",
            "--- Generating with seed: \"eah. oh, can i come?\n",
            "monica: yeah. rach, are you thinking yo\"\n",
            "------ temperature: 0.1\n",
            "eah. oh, can i come?\n",
            "monica: yeah. rach, are you thinking your a friend.\n",
            "ross: i don't know what you wanna be a brought me on the table. i mean, i mean, i mean,\n",
            "------ temperature: 0.5\n",
            " wanna be a brought me on the table. i mean, i mean, i mean, it was for a be a coffee in the distle break\n",
            "[scene: central perk, the to a spack of the daye the b\n",
            "------ temperature: 0.8\n",
            "break\n",
            "[scene: central perk, the to a spack of the daye the baby, so happened bell. daringen insed'll you. he's not with right! i mean, i'm even fives in a think\n",
            "epoch 16\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.1681\n",
            "--- Generating with seed: \"d shrug. ross enters with marcel on his shoulder)\n",
            "ross: par-\"\n",
            "------ temperature: 0.1\n",
            "d shrug. ross enters with marcel on his shoulder)\n",
            "ross: par-this with a little play of the catw and i was a little work.\n",
            "rachel: (to ross) well, i don't know wh\n",
            "------ temperature: 0.5\n",
            "i was a little work.\n",
            "rachel: (to ross) well, i don't know what we stares in the cat, in the phone, i don't know i have a like the were this what i don't know i \n",
            "------ temperature: 0.8\n",
            " don't know i have a like the were this what i don't know i was you get a way i wants here, and you guys going at about your back of your a sweep.\n",
            "rachel: so ro\n",
            "epoch 17\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 1.1610\n",
            "--- Generating with seed: \" dragging his hands...\n",
            "chandler: that's so weird, i had such\"\n",
            "------ temperature: 0.1\n",
            " dragging his hands...\n",
            "chandler: that's so weird, i had such the bast of the other to the too breaks the store of the store of the cate to the store of the cate\n",
            "------ temperature: 0.5\n",
            " the store of the store of the cate to the store of the cate momsta enters. chandler and joey is take out lading to the bast room.)\n",
            "ross: god, and the minnied t\n",
            "------ temperature: 0.8\n",
            "e out lading to the bast room.)\n",
            "ross: god, and the minnied the pug to to get there, bat some party, stewing them gods.\n",
            "phoebe: oh, i think i mean, you'd mom.\n",
            "jo\n",
            "epoch 18\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 1.1510\n",
            "--- Generating with seed: \"ofabed a tiny push and it folds away)\n",
            "joey: hold on, you-you\"\n",
            "------ temperature: 0.1\n",
            "ofabed a tiny push and it folds away)\n",
            "joey: hold on, you-you don't want to be a bearite that was a break you are again!\n",
            "ross: what about that was a break your b\n",
            "------ temperature: 0.5\n",
            "reak you are again!\n",
            "ross: what about that was a break your back to the hall.\n",
            "rachel: he's not that was you was a something here?\n",
            "ross: what are you doing this g\n",
            "------ temperature: 0.8\n",
            "as you was a something here?\n",
            "ross: what are you doing this great mefor in a lefor there with the table, bat oary in the candy intelals acroom in the cartabot al\n",
            "epoch 19\n",
            "Epoch 1/1\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 1.1451\n",
            "--- Generating with seed: \"t! this is what i'm doing now. i've got this job-\n",
            "kiki: wait\"\n",
            "------ temperature: 0.1\n",
            "t! this is what i'm doing now. i've got this job-\n",
            "kiki: wait, it's like that the birthing what you were me is that so that we work you were this?\n",
            "chandler: you \n",
            "------ temperature: 0.5\n",
            "ere me is that so that we work you were this?\n",
            "chandler: you know, that-whot was this?\n",
            "chandler: you know, i have three time to be with.\n",
            "joey: (secket it and rac\n",
            "------ temperature: 0.8\n",
            "know, i have three time to be with.\n",
            "joey: (secket it and rachel ssees monica are conver table of kesset fire, and carm than you, use, how lovg nzey of thmmen in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFPK7-bbFmFP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "39e9713d-fa93-4bfa-d5bc-8e997c1567a2"
      },
      "source": [
        "for epoch in range(1, 3):\n",
        "    print('epoch', epoch)\n",
        "    # Fit the model for 1 epoch on the available training data\n",
        "    model.fit(x, y,\n",
        "              batch_size=128,\n",
        "              epochs=30)\n",
        "\n",
        "    # Select a text seed at random\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated_text = text[start_index: start_index + maxlen]\n",
        "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "    for temperature in [0.1, 0.5, 0.8,]:\n",
        "        print('------ temperature:', temperature)\n",
        "        sys.stdout.write(generated_text)\n",
        "\n",
        "        # We generate 100 characters\n",
        "        for i in range(100):\n",
        "            sampled = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(generated_text):\n",
        "                sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(sampled, verbose=0)[0]\n",
        "            next_index = sample(preds, temperature)\n",
        "            next_char = chars[next_index]\n",
        "\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "Epoch 1/30\n",
            "99980/99980 [==============================] - 130s 1ms/step - loss: 0.9971\n",
            "Epoch 2/30\n",
            "99980/99980 [==============================] - 130s 1ms/step - loss: 0.9915\n",
            "Epoch 3/30\n",
            "99980/99980 [==============================] - 130s 1ms/step - loss: 0.9875\n",
            "Epoch 4/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9838\n",
            "Epoch 5/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9829\n",
            "Epoch 6/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9836\n",
            "Epoch 7/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9783\n",
            "Epoch 8/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9769\n",
            "Epoch 9/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9730\n",
            "Epoch 10/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9751\n",
            "Epoch 11/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9743\n",
            "Epoch 12/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9721\n",
            "Epoch 13/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9675\n",
            "Epoch 14/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9682\n",
            "Epoch 15/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9662\n",
            "Epoch 16/30\n",
            "99980/99980 [==============================] - 130s 1ms/step - loss: 0.9629\n",
            "Epoch 17/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9611\n",
            "Epoch 18/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9614\n",
            "Epoch 19/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9559\n",
            "Epoch 20/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9545\n",
            "Epoch 21/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9548\n",
            "Epoch 22/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9565\n",
            "Epoch 23/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9555\n",
            "Epoch 24/30\n",
            "99980/99980 [==============================] - 133s 1ms/step - loss: 0.9518\n",
            "Epoch 25/30\n",
            "99980/99980 [==============================] - 133s 1ms/step - loss: 0.9505\n",
            "Epoch 26/30\n",
            "99980/99980 [==============================] - 133s 1ms/step - loss: 0.9472\n",
            "Epoch 27/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9472\n",
            "Epoch 28/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9453\n",
            "Epoch 29/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9432\n",
            "Epoch 30/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9455\n",
            "--- Generating with seed: \"ey, you guys in the living room all know what you want to do\"\n",
            "------ temperature: 0.1\n",
            "ey, you guys in the living room all know what you want to do this not the gradle thing with a big closed, but ross and rachel are still talking to the to the do\n",
            "------ temperature: 0.5\n",
            "osed, but ross and rachel are still talking to the to the door waiter sick that and i was the graned on a bag. i just all uh. (reaches around to her hist finger\n",
            "------ temperature: 0.8\n",
            " on a bag. i just all uh. (reaches around to her hist finger her with and gect"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ure rachel) sorryaticalice.\n",
            "chandler: i'm sorry, bint.\n",
            "chandler: oh.\n",
            "janicay, list\n",
            "epoch 2\n",
            "Epoch 1/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9442\n",
            "Epoch 2/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9412\n",
            "Epoch 3/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9353\n",
            "Epoch 4/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9377\n",
            "Epoch 5/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9373\n",
            "Epoch 6/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9375\n",
            "Epoch 7/30\n",
            "99980/99980 [==============================] - 129s 1ms/step - loss: 0.9332\n",
            "Epoch 8/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9317\n",
            "Epoch 9/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9336\n",
            "Epoch 10/30\n",
            "99980/99980 [==============================] - 128s 1ms/step - loss: 0.9319\n",
            "Epoch 11/30\n",
            "99980/99980 [==============================] - 136s 1ms/step - loss: 0.9335\n",
            "Epoch 12/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9311\n",
            "Epoch 13/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9292\n",
            "Epoch 14/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9293\n",
            "Epoch 15/30\n",
            "99980/99980 [==============================] - 133s 1ms/step - loss: 0.9282\n",
            "Epoch 16/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9269\n",
            "Epoch 17/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9260\n",
            "Epoch 18/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9255\n",
            "Epoch 19/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9236\n",
            "Epoch 20/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9218\n",
            "Epoch 21/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9215\n",
            "Epoch 22/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9179\n",
            "Epoch 23/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9174\n",
            "Epoch 24/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9164\n",
            "Epoch 25/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9173\n",
            "Epoch 26/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9187\n",
            "Epoch 27/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9169\n",
            "Epoch 28/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9130\n",
            "Epoch 29/30\n",
            "99980/99980 [==============================] - 132s 1ms/step - loss: 0.9132\n",
            "Epoch 30/30\n",
            "99980/99980 [==============================] - 131s 1ms/step - loss: 0.9131\n",
            "--- Generating with seed: \"a what you're thinking. if you don't ask her out soon you're\"\n",
            "------ temperature: 0.1\n",
            "a what you're thinking. if you don't ask her out soon you're going to stan to tell you.\n",
            "monica: oh, i don't know. i have to do it with you guys what i don't kno\n",
            "------ temperature: 0.5\n",
            "i don't know. i have to do it with you guys what i don't know. i have to able.\n",
            "(rachel and rachel) what's it is still say paolo's face i was a bug in a call doi\n",
            "------ temperature: 0.8\n",
            "hat's it is still say paolo's face i was a bug in a call doing!\n",
            "rachel: i don't think i what i permm fing of pre-hew!'.. it's in his life, and i always got a li\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wekDg4Tzm2i",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2W3_d0neNht",
        "colab_type": "text"
      },
      "source": [
        "to do:\n",
        "\n",
        "English lyrics as corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0nu8vQQiirV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}